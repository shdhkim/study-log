# 머신러닝 핵심 개념 정리

## 앙상블 기법

### 배깅 (Bagging)
- **같은 모델**을 **여러 번 훈련**
- 각기 다른 데이터 샘플로 학습 후 예측 결과 **평균** 또는 **투표**
- 예: **랜덤 포레스트**
- 비유: 한 전문가가 다양한 관점으로 여러 번 생각하고 평균을 냄

### 보팅 (Voting)
- **서로 다른 모델**들의 예측을 결합
- 모든 모델이 예측한 결과를 기반으로 **다수결(투표)**

#### 하드 보팅 (Hard Voting)
- 각 모델의 **최종 예측값** 기준으로 **가장 많이 나온 클래스 선택**

#### 소프트 보팅 (Soft Voting)
- 각 모델의 **예측 확률 평균**을 내고, **가장 높은 확률의 클래스 선택**
- 비유: 서로 다른 3명의 전문가가 각각 의견을 내고 다수결로 결정

---

## 선형 vs 비선형

### 비선형 (Non-linear)
- 변수 간 관계가 **직선적이지 않거나 비례적이지 않음**
- 대부분의 실제 문제는 **비선형 문제**임

---

## 신경망 기초

### 퍼셉트론 (Perceptron)
- 단일 신경세포 모델
- 입력에 가중치를 곱한 뒤, 활성화 함수(step 함수 등)를 거쳐 출력 결정
- **선형 분리 문제만 해결 가능**
  - 예: AND, OR 가능 / XOR 불가능

### 다중 퍼셉트론 (MLP, Multi-Layer Perceptron)
- 퍼셉트론을 **여러 층으로 구성**
- 구조: 입력층 → 은닉층(hidden layer) → 출력층
- 각 층의 뉴런은 **비선형 활성화 함수** 사용 (예: ReLU, Sigmoid)
- **비선형 문제도 해결 가능**
  - 예: XOR 문제도 해결 가능

---

## 순환 신경망 (RNN)

### RNN (Recurrent Neural Network)
- 순차적 데이터 처리에 특화된 인공신경망
- 이전 단계의 출력이 현재 입력에 영향을 줌 → **메모리 기능**
- **활용 분야**:
  - 문장 처리 (자연어 처리)
  - 음성 인식
  - 주가 예측
  - 시계열 데이터 분석

### LSTM (Long Short-Term Memory)
- RNN의 **장기 기억 한계**를 보완한 구조
- 오랜 시간 이전 정보도 잘 기억할 수 있도록 **게이트 구조 도입**
  - 입력 게이트 / 출력 게이트 / 망각 게이트
- **장기 의존성 문제 해결**에 효과적