# **Transformer: NLP의 핵심 구조**
- **Transformer**: NLP의 핵심 구조 (**인코더 + 디코더**)
- **BERT**: 인코더 기반 모델로 **문맥 이해에 강함**
- **GPT**: 디코더 기반 모델로 **텍스트 생성에 강함**

---

# **과적합(Overfitting)**
과적합이란 **머신러닝 모델이 훈련 데이터에 지나치게 적합하여**,  
새로운 데이터(테스트 데이터 또는 실전 환경)에서 **일반화 성능이 떨어지는 현상**을 의미함.  
모델이 학습 데이터의 패턴뿐만 아니라 **노이즈까지 학습해버려**,  
실제 예측 시 오히려 성능이 저하되는 문제가 발생함.

## **노이즈(Noise)**
데이터에 포함된 **불필요한 정보**나 **랜덤한 변동 요소**를 의미함.  
노이즈는 머신러닝과 데이터 분석에서 학습 과정에 방해가 될 수 있으며,  
모델이 과적합되는 주요 원인 중 하나임.

### **적절한 학습 (좋은 모델)**
> 문제집에서 개념을 이해하고 다양한 유형을 연습한 후,  
> 시험에서 처음 보는 문제도 잘 푸는 경우  
✅ **일반화가 잘된 모델**

### **과적합 (Overfitting)**
> 문제집의 모든 문제와 답을 통째로 외워서 훈련 문제는 잘 맞히지만,  
> 새로운 문제가 나오면 풀지 못하는 경우  
❌ **훈련 데이터에 너무 맞춰져 있어서 실제 테스트에서 성능이 떨어지는 모델**

### **과소적합 (Underfitting)**
> 개념도 제대로 이해하지 않고 대충 공부해서,  
> 문제집에서도 성적이 안 나오고 시험에서도 못 보는 경우  
❌ **너무 단순한 모델, 학습이 부족한 상태**

---

# **노이즈 유형**
| 유형                | 설명 |
|--------------------|-----------------------------------------------------------|
| **측정 오류(Measurement Error)** | 데이터 수집 과정에서 발생하는 오차 (센서 오류, 입력 실수 등) |
| **랜덤 노이즈(Random Noise)** | 실제 데이터와 관계없는 우연적인 변동 요소 (예: 주식 시장 단기 변동) |
| **구조적 노이즈(Structural Noise)** | 잘못된 가정이나 모델링에서 발생 (예: 잘못된 변수 선택, 부정확한 모델 설계) |
| **태깅 오류(Label Noise)** | 잘못된 라벨이 달린 데이터 (예: 스팸이 아닌 메일을 스팸으로 분류) |

---

# **KNN(K-Nearest Neighbors)**
- 지도 학습(Supervised Learning) 방법 중 하나.
- 특정 데이터를 예측할 때 가장 가까운 **K개의 데이터를 참고하여** 분류(Classification) 또는 회귀(Regression) 수행.

---

# **뉴런(Neuron)과 신경망**
- **뉴런(Neuron)**: **인공신경망(Artificial Neural Network, ANN)의 기본 단위**.
- 인간의 뇌에서 신호를 주고받는 **뉴런(신경 세포)**을 모방하여 만들어진 연산 단위.

## **뉴런이 모이면? → 뉴럴 네트워크 (Neural Network)**
- 뉴런 하나만으로는 복잡한 패턴을 학습할 수 없음.
- 여러 개의 뉴런을 **층(Layer)** 형태로 쌓아서 **인공신경망(Neural Network)** 형성.

### **인공신경망의 구조**
1. **입력층 (Input Layer)**: 원본 데이터를 입력받음.
2. **은닉층 (Hidden Layer)**: 여러 개의 뉴런이 서로 연결되어 **패턴을 학습**.
   - **DNN(Deep Neural Network)**에서는 은닉층이 많아짐.
3. **출력층 (Output Layer)**: 최종 결과값을 출력.

---

# **벡터 데이터베이스(Vector DB)**
- **벡터 형식으로 저장된 데이터에 대해 효율적인 검색과 유사성 탐색(NNS)을 지원하는 데이터베이스**.
- 기존 **RDB**나 **NoSQL**과 달리, **벡터 임베딩(embedding)**을 기반으로 데이터 검색을 수행.
- AI, 머신러닝, 추천 시스템, NLP 등의 분야에서 유사도 기반 검색에 활용됨.

---

# **멀티 헤드 어텐션(Multi-Head Attention, MHA)**
- **트랜스포머(Transformer) 모델의 핵심 기법**.
- 여러 개의 어텐션 연산을 병렬로 수행하여 **모델이 다양한 문맥 정보를 학습할 수 있도록 도와줌**.

## **멀티 헤드 어텐션이 필요한 이유?**
### ❌ **문제점: 단일 어텐션의 한계**
- **싱글 헤드 어텐션(Single-Head Attention)**은 한 번만 어텐션을 수행하므로,  
  **문맥을 하나의 관점에서만 이해할 수 있음**.

### ✅ **해결책: 멀티 헤드 어텐션**
- 여러 개의 어텐션을 병렬로 수행하여, 단어 간의 관계를 **다양한 관점에서 학습**.
- 각 헤드는 **다른 방식으로 문맥을 분석 가능**.

---

# **GPT 모델의 파라미터(Parameter)**
- **파라미터란?** 모델이 학습하는 값(weight, bias)으로, 뉴런의 연결 강도를 조정하는 역할.
- GPT 모델의 **파라미터 수가 많을수록 더 복잡한 패턴을 학습**하고 더 자연스러운 문장을 생성 가능.

### **GPT 모델의 텍스트 생성 과정**
1. **입력 문장을 벡터화 (Embedding)**
2. **트랜스포머 디코더에서 여러 층을 거쳐 문맥 학습**
3. **멀티 헤드 어텐션을 이용해 중요한 관계 학습**
4. **피드포워드 네트워크(FFN)에서 정보 조합**
5. **출력 레이어에서 다음 단어 예측**

---

# **선형 변환(Linear) vs 비선형 변환(Non-Linear)**
### **선형 변환 (Linear Transformation)**
- **행렬 곱(Matrix Multiplication)과 바이어스(Bias) 추가만 수행**.
- **수식:** `h = XW + b`
- ❌ **문제점:** 선형 변환만 사용하면 모델이 **단순한 관계(직선 관계)만 학습**하여, 복잡한 패턴을 표현할 수 없음.

### **비선형 변환 (Non-Linear Transformation)**
- **비선형 활성화 함수(Activation Function)를 적용하여 복잡한 관계를 학습할 수 있도록 함**.
- **수식:** `h = f(XW + b)`
- **활성화 함수 예시**: ReLU, GELU, Sigmoid 등.

---