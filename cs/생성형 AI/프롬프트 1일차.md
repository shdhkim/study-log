##  파라미터(Parameter) vs 하이퍼파라미터(Hyperparameter)

###  파라미터
- 모델이 **학습 과정에서 데이터로부터 자동으로 학습**하는 값들  
- 예시: **가중치(Weight), 편향(Bias)**  
- 학습 데이터 + 손실 함수 + 최적화 알고리즘(ex. 경사 하강법)을 통해 업데이트됨

###  하이퍼파라미터
- 모델이 직접 학습하지 않고, **사용자가 설정해야 하는 값들**  
- 예시: **학습률(Learning Rate), 은닉층 수/크기, 배치 크기, 정규화 계수, 드롭아웃 비율**  
- 모델의 구조나 학습 방식 전반에 영향을 미침

---

##  생성형 AI란?

- **기존 학습 데이터의 패턴과 구조**를 바탕으로  
- **텍스트, 이미지, 음성, 영상 등 새로운 콘텐츠를 생성**하는 기술  
- 예시: **GPT 계열(텍스트), DALL·E(이미지)**

학습한 **복잡한 확률 분포나 패턴**을 바탕으로 창작

---

##  범주형 데이터 인코딩

###  One-hot 인코딩
- 범주형 데이터를 0과 1로 이루어진 벡터로 변환  
- 예: [‘사과’, ‘바나나’, ‘포도’] → [1, 0, 0]

###  Bag of Words (BoW)
- 문장/문서 내 단어들의 **등장 횟수 기반 벡터화**  
- 문맥, 순서 **고려하지 않음**

---

##  TF-IDF (Term Frequency - Inverse Document Frequency)

###  TF (단어 빈도)
- 특정 문서에서 단어가 **얼마나 자주 등장**했는지

###  IDF (역문서 빈도)
- 단어가 **전체 문서에서 얼마나 드물게 등장**하는지를 측정  
- 많이 나오는 단어는 **덜 중요**하다고 판단

> 단어의 중요도를 수치화하여 반영

---

##  n-gram

- 문장을 **n개의 연속된 요소**(단어 또는 문자)로 나누는 기법

예시:  
- 1-gram: [‘나는’, ‘너를’, ‘사랑해’]  
- 2-gram: [‘나는 너를’, ‘너를 사랑해’]

---

##  대표적인 Word Embedding 기법

| 방법         | 설명 |
|--------------|------|
| **Word2Vec** | 구글이 개발. 주변 단어로부터 의미 파악 |
| **GloVe**    | 스탠포드 개발. 전체 통계 기반 |
| **FastText** | 페이스북 개발. 단어 내부 subword까지 학습 |
| **BERT**     | 문맥 반영형 벡터. Transformer 기반 |

---

##  Sparse vs Dense Vector

###  Sparse Vector (희소 벡터)
- 예: One-hot, BoW  
- 대부분이 0 → **공간 낭비**  
- 단어 간 유사성 **미반영**

###  Dense Vector (밀집 벡터)
- 예: Word2Vec, BERT  
- 대부분 값이 0 아님  
- 유사한 단어 간 거리가 가까움 → **의미 반영**

---

##  텍스트 생성 모델 (RNN 기반)

###  학습 단계
- 입력: "나는 밥을"  
- 정답: "먹었다"  
- 수많은 문장 반복 학습 → 언어 패턴 습득

###  예측 단계
- “나는 밥을” 입력 시 → 모델은 “먹었다” 예측

---

##  LSTM (Long Short-Term Memory)

- 기존 RNN의 한계(기억력 부족)를 보완  
- 구성 요소: 3개의 게이트

| 게이트         | 역할                             |
|----------------|----------------------------------|
| **Forget Gate**| 과거 정보 중 **잊을 것** 결정    |
| **Input Gate** | 새로운 정보 중 **기억할 것** 결정 |
| **Output Gate**| 현재 시점에서 **출력할 값** 결정 |

---

##  GRU (Gated Recurrent Unit)

- LSTM보다 **구조 간단**, 연산 효율 높음  
- 구성 요소: 2개의 게이트

| 게이트          | 역할                             |
|-----------------|----------------------------------|
| **Update Gate** | LSTM의 Forget + Input 역할 통합 |
| **Reset Gate**  | 이전 정보 반영 정도 결정         |

---

##  인코더 & Transformer 인코더

###  인코딩
- 책 스캔 → 텍스트를 **디지털 숫자**로 바꾸는 행위

###  인코더
- 스캔된 텍스트를 읽고 **요약**하는 기능

###  Transformer 인코더
- **Self-Attention**을 통해 **전체 문맥을 동시에 이해**하며 요약

---